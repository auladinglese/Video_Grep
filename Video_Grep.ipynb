{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Clip Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Requires\n",
    "  - nltk\n",
    "    - stopwords('english')\n",
    "    - RegexpTokenizer\n",
    "  - pip install srt\n",
    "  - pip install webvtt-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import srt\n",
    "from nltk.tokenize import RegexpTokenizer # punctuation removal and tokenizing subtitle block\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import webvtt # for captioning in HTML playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path where video and subtitle files are stored\n",
    "#path = 'D:\\\\captures\\\\NPTEL\\\\'\n",
    "path = 'D:\\\\captures\\\\NPTEL\\\\ANLP\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively search for MP4 and SRT files\n",
    "file_types = ['*.mp4', '*.srt'] # the tuple of file types\n",
    "files_grabbed = [glob.glob(path + '**/'+ e, recursive=True) for e in file_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[62, 28]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(len,files_grabbed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary out of grabbed files\n",
    "files_dict = defaultdict(list)\n",
    "for f_type in files_grabbed:\n",
    "    for f in f_type:\n",
    "        v = f.split('.')\n",
    "        files_dict[v[0]].append(v[1])\n",
    "\n",
    "# Remove entries that do not have either .mp4 or .srt file\n",
    "remove_list = [key for key in files_dict if(len(files_dict[key]) < len(file_types))]\n",
    "for k in remove_list:\n",
    "    del files_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_srt_to_vtt(srt_file_name,webvtt=webvtt):\n",
    "    '''\n",
    "    Input: Subtile file name which is in SRT file format (.srt)\n",
    "    Output: Subtitle file in OTT file format (.vtt)\n",
    "    Requires: webvtt module imported already\n",
    "    '''\n",
    "    webvtt = webvtt.from_srt(srt_file_name)\n",
    "    webvtt.save()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML5 supports only VTT caption format\n",
    "# In order to play video with caption, we need VTT file\n",
    "# So generate WebVTT file from available SRT files\n",
    "for file_name in files_dict:\n",
    "    srt_file_name = file_name+'.srt'\n",
    "    convert_srt_to_vtt(srt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dict to pandas dataframe for easy manipulation\n",
    "df = pd.DataFrame(columns=['file','mp4', 'srt'])\n",
    "for index, (key, val) in enumerate(files_dict.items()):\n",
    "    df.loc[index] = [key.replace(path,'')] + val\n",
    "    #df.loc[index] = [key] + val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>mp4</th>\n",
       "      <th>srt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Week1\\mod01lec04-Vector Space models</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Week1\\mod01lec06-Machine Translation</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Week1\\mod01lec07-Preprocessing</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Week1\\mod01lec09-Statistical Properties of Wor...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Week2\\mod02lec15-Co-occurence matrix, n-grams</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Week2\\mod02lec16-Collocations, Dense word Vectors</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Week2\\mod02lec17-SVD, Dimensionality reduction...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Week3\\mod03lec20-Examples for word prediction</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Week3\\mod03lec21-Introduction to Probability i...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Week3\\mod03lec23-The definition of probabilist...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Week3\\mod03lec24-Chain rule and Markov assumption</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Week3\\mod03lec25-Generative Models</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Week3\\mod03lec27-Out of vocabulary words and c...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Week3\\mod03lec29-Naive-Bayes Algorithm for cla...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Week4\\mod04lec34-Perceptron Learning</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>Week4\\mod04lec35-Logical XOR</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>Week4\\mod04lec37-Gradient Descent</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>Week5\\mod05lec38-Feedforward and Back propagat...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Week5\\mod05lec40-What are CBOW and Skip-Gram M...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>Week5\\mod05lec41-One word learning architecture</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>Week5\\mod05lec43-Matrix Operations Explained</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>Week6\\mod06lec45-Building Skip-gram model usin...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>Week6\\mod06lec53-Sequence Learning and its app...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>Week7\\mod07lec54-Introuduction to Recurrent Ne...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>Week7\\mod07lec55-Unrolled RNN</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>Week7\\mod07lec56-RNN - Based Language Model</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>Week7\\mod07lec59-BPTT - Exploding and vanishin...</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>Week7\\mod07lec62-GRU</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 file  mp4  srt\n",
       "0                Week1\\mod01lec04-Vector Space models  mp4  srt\n",
       "1                Week1\\mod01lec06-Machine Translation  mp4  srt\n",
       "2                      Week1\\mod01lec07-Preprocessing  mp4  srt\n",
       "3   Week1\\mod01lec09-Statistical Properties of Wor...  mp4  srt\n",
       "4       Week2\\mod02lec15-Co-occurence matrix, n-grams  mp4  srt\n",
       "5   Week2\\mod02lec16-Collocations, Dense word Vectors  mp4  srt\n",
       "6   Week2\\mod02lec17-SVD, Dimensionality reduction...  mp4  srt\n",
       "7       Week3\\mod03lec20-Examples for word prediction  mp4  srt\n",
       "8   Week3\\mod03lec21-Introduction to Probability i...  mp4  srt\n",
       "9   Week3\\mod03lec23-The definition of probabilist...  mp4  srt\n",
       "10  Week3\\mod03lec24-Chain rule and Markov assumption  mp4  srt\n",
       "11                 Week3\\mod03lec25-Generative Models  mp4  srt\n",
       "12  Week3\\mod03lec27-Out of vocabulary words and c...  mp4  srt\n",
       "13  Week3\\mod03lec29-Naive-Bayes Algorithm for cla...  mp4  srt\n",
       "14               Week4\\mod04lec34-Perceptron Learning  mp4  srt\n",
       "15                       Week4\\mod04lec35-Logical XOR  mp4  srt\n",
       "16                  Week4\\mod04lec37-Gradient Descent  mp4  srt\n",
       "17  Week5\\mod05lec38-Feedforward and Back propagat...  mp4  srt\n",
       "18  Week5\\mod05lec40-What are CBOW and Skip-Gram M...  mp4  srt\n",
       "19    Week5\\mod05lec41-One word learning architecture  mp4  srt\n",
       "20       Week5\\mod05lec43-Matrix Operations Explained  mp4  srt\n",
       "21  Week6\\mod06lec45-Building Skip-gram model usin...  mp4  srt\n",
       "22  Week6\\mod06lec53-Sequence Learning and its app...  mp4  srt\n",
       "23  Week7\\mod07lec54-Introuduction to Recurrent Ne...  mp4  srt\n",
       "24                      Week7\\mod07lec55-Unrolled RNN  mp4  srt\n",
       "25        Week7\\mod07lec56-RNN - Based Language Model  mp4  srt\n",
       "26  Week7\\mod07lec59-BPTT - Exploding and vanishin...  mp4  srt\n",
       "27                               Week7\\mod07lec62-GRU  mp4  srt"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>mp4</th>\n",
       "      <th>srt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>Week1\\mod01lec04-Vector Space models</td>\n",
       "      <td>mp4</td>\n",
       "      <td>srt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        file  mp4  srt\n",
       "count                                     28   28   28\n",
       "unique                                    28    1    1\n",
       "top     Week1\\mod01lec04-Vector Space models  mp4  srt\n",
       "freq                                       1   28   28"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 28 entries, 0 to 27\n",
      "Data columns (total 3 columns):\n",
      "file    28 non-null object\n",
      "mp4     28 non-null object\n",
      "srt     28 non-null object\n",
      "dtypes: object(3)\n",
      "memory usage: 896.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Week1\\mod01lec04-Vector Space models\n"
     ]
    }
   ],
   "source": [
    "print(df.loc[0]['file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D:\\captures\\NPTEL\\Cloud Computing\\Lecture 11_-mod03lec11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stop_words = ['ah', 'music']\n",
    "stop_words = set(stopwords.words('english') + custom_stop_words)\n",
    "\n",
    "def pre_process_srt_block_content(doc):\n",
    "    '''\n",
    "    preprocesses the given document\n",
    "    and returns it as a single string\n",
    "    '''\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "\n",
    "    #translator = str.maketrans('', '', string.punctuation)\n",
    "    #doc = doc.translate(translator)\n",
    "    \n",
    "    #tokens = doc.split() # split it into words\n",
    "    tokens = [token.lower() for token in tokens] # normalize\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens\n",
    "    #doc_str = ' '.join(tokens)\n",
    "    #return doc_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SrtDocument:\n",
    "    '''\n",
    "    Contains SRT Block information\n",
    "    start : SRT block start time  in datetime.timedelta format\n",
    "    end : SRT block start time  in datetime.timedelta format\n",
    "    content : SRT block as a single string\n",
    "    '''\n",
    "    __slots__ = ('start', 'end', 'tokens','content_str')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ' '.join (\n",
    "                         [\n",
    "                             '{', \n",
    "                            'start:', str(self.start),\n",
    "                            'end:', str(self.end),\n",
    "                            'content_str:', self.content,\n",
    "                            '}'\n",
    "                           ])\n",
    "    def __str__(self):\n",
    "        return ' '.join([str(self.start), str(self.end), self.content_str])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_srt_file(srt_file):\n",
    "    '''\n",
    "    Input:\n",
    "    srt_file: Subtile file opened in read-only mode\n",
    "    \n",
    "    Actions:\n",
    "    Parse the SRT file\n",
    "    converts each SRT block into a single string\n",
    "    preprocess's the string and return it\n",
    "    \n",
    "    Returns:\n",
    "        Generator providing SRT blocks in SrtDocument format\n",
    "    '''\n",
    "    subs = srt.parse(f_srt)\n",
    "    for subtitle in subs:\n",
    "        #print(subtitle)\n",
    "        #print(subtitle.content.split())\n",
    "        #print(' '.join(subtitle.content.split()))\n",
    "        doc_content = pre_process_srt_block_content(subtitle.content)\n",
    "        if(len(doc_content) == 0): # remove empty entries\n",
    "            continue\n",
    "        srt_doc = SrtDocument()\n",
    "        srt_doc.start = subtitle.start\n",
    "        srt_doc.end = subtitle.end\n",
    "        srt_doc.tokens = doc_content\n",
    "        srt_doc.content_str = ' '.join(doc_content)\n",
    "        yield srt_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:14.620000 0:00:24.279000 fourth one really understand\n",
      "0:00:18.849000 0:00:27.340000 words words\n",
      "0:00:24.279000 0:00:30.189000 convey right something\n",
      "0:00:27.340000 0:00:35.800000 need understand order us\n",
      "0:00:30.189000 0:00:40.750000 go next level right\n",
      "0:00:35.800000 0:00:43.420000 want find really\n",
      "0:00:40.750000 0:00:47.739000 certain mathematical operations\n",
      "0:00:43.420000 0:00:50.129000 text really convert text\n",
      "0:00:47.739000 0:00:57.030000 vector form use\n",
      "0:00:50.129000 0:01:00.969000 vector algebra certain vector\n",
      "0:00:57.030000 0:01:04.030000 algebraic methods find certain inner\n",
      "0:01:00.969000 0:01:05.800000 meanings find document\n",
      "0:01:04.030000 0:01:09.909000 related looking\n",
      "0:01:05.800000 0:01:13.510000 give one idea let suppose\n",
      "0:01:09.909000 0:01:16.810000 document collection containing\n",
      "0:01:13.510000 0:01:21.070000 billion words ten\n",
      "0:01:16.810000 0:01:23.520000 thousand documents search\n",
      "0:01:21.070000 0:01:26.799000 mechanism find\n",
      "0:01:23.520000 0:01:29.740000 document let assume\n",
      "0:01:26.799000 0:01:35.350000 search engine running behind\n",
      "0:01:29.740000 0:01:37.750000 give word body right\n",
      "0:01:35.350000 0:01:39.310000 normal search\n",
      "0:01:37.750000 0:01:41.110000 engine go look\n",
      "0:01:39.310000 0:01:44.590000 word buy get\n",
      "0:01:41.110000 0:01:48.549000 documents word occurred right\n",
      "0:01:44.590000 0:01:50.170000 lists based ranking\n",
      "0:01:48.549000 0:01:53.680000 let worry ranking right\n",
      "0:01:50.170000 0:01:56.200000 let let see listed\n",
      "0:01:53.680000 0:01:59.109000 certain documents certain order\n",
      "0:01:56.200000 0:02:05.500000 every document word\n",
      "0:01:59.109000 0:02:08.500000 right want get\n",
      "0:02:05.500000 0:02:12.899000 level also want find document\n",
      "0:02:08.500000 0:02:12.899000 contains words\n",
      "0:02:20.930000 0:02:30.650000 started right\n",
      "0:02:26.840000 0:02:33.650000 rudimentary search engine\n",
      "0:02:30.650000 0:02:38.420000 gave certain results\n",
      "0:02:33.650000 0:02:44.150000 words document contained\n",
      "0:02:38.420000 0:02:47.299000 word listed certain order\n",
      "0:02:44.150000 0:02:50.090000 list anything related\n",
      "0:02:47.299000 0:02:53.239000 birth documents contain\n",
      "0:02:50.090000 0:02:55.750000 two words right\n",
      "0:02:53.239000 0:03:00.170000 next step possible\n",
      "0:02:55.750000 0:03:02.120000 get things done certain way\n",
      "0:03:00.170000 0:03:06.769000 one simple way\n",
      "0:03:02.120000 0:03:08.750000 use either stemming limit\n",
      "0:03:06.769000 0:03:10.250000 isolation let talk\n",
      "0:03:08.750000 0:03:13.700000 talk details little\n",
      "0:03:10.250000 0:03:16.299000 later give word\n",
      "0:03:13.700000 0:03:16.299000 bhat\n",
      "0:03:24.400000 0:03:37.900000 converted look\n",
      "0:03:35.769000 0:03:42.250000 verbs fashion\n",
      "0:03:37.900000 0:03:45.280000 convert root\n",
      "0:03:42.250000 0:03:48.930000 normalizing entire corpus wherever\n",
      "0:03:45.280000 0:03:51.160000 seen word\n",
      "0:03:48.930000 0:03:53.170000 replaced word bye\n",
      "0:03:51.160000 0:03:56.200000 buying would case\n",
      "0:03:53.170000 0:04:02.140000 possible us get\n",
      "0:03:56.200000 0:04:03.880000 results contain word buy\n",
      "0:04:02.140000 0:04:07.750000 one way\n",
      "0:04:03.880000 0:04:09.459000 good could\n",
      "0:04:07.750000 0:04:10.959000 lot document incoming\n",
      "0:04:09.459000 0:04:13.989000 know expect everybody\n",
      "0:04:10.959000 0:04:17.440000 keep processing step\n",
      "0:04:13.989000 0:04:20.260000 also really nice know\n",
      "0:04:17.440000 0:04:22.990000 way learn language\n",
      "0:04:20.260000 0:04:26.650000 especially trying\n",
      "0:04:22.990000 0:04:28.990000 make mission learn\n",
      "0:04:26.650000 0:04:32.110000 ways possible\n",
      "0:04:28.990000 0:04:35.289000 use vector space model find\n",
      "0:04:32.110000 0:04:37.500000 distance buy bought\n",
      "0:04:35.289000 0:04:39.970000 buying close enough\n",
      "0:04:37.500000 0:04:42.820000 bring words close\n",
      "0:04:39.970000 0:04:46.240000 enough buy get\n",
      "0:04:42.820000 0:04:52.979000 documents words close\n",
      "0:04:46.240000 0:04:56.349000 enough write find\n",
      "0:04:52.979000 0:04:57.940000 using mechanism going\n",
      "0:04:56.349000 0:05:03.039000 possible list\n",
      "0:04:57.940000 0:05:08.470000 document list\n",
      "0:05:03.039000 0:05:10.720000 document\n",
      "0:05:08.470000 0:05:13.570000 possible list document\n",
      "0:05:10.720000 0:05:16.260000 contain word\n",
      "0:05:13.570000 0:05:21.639000 also list documents\n",
      "0:05:16.260000 0:05:23.590000 contain words n correct\n",
      "0:05:21.639000 0:05:26.110000 something want\n",
      "0:05:23.590000 0:05:29.020000 achieve possible really get\n",
      "0:05:26.110000 0:05:31 certain fashion vector\n",
      "0:05:29.020000 0:05:32.139000 space model words would really help\n",
      "0:05:31 0:05:34.540000 us going\n",
      "0:05:32.139000 0:05:37.360000 looking\n",
      "0:05:34.540000 0:05:41.980000 let us place model way want\n",
      "0:05:37.360000 0:05:44.550000 reduce word would represented\n",
      "0:05:41.980000 0:05:48.340000 one axis example\n",
      "0:05:44.550000 0:05:50.830000 seven thousand seventy nine words\n",
      "0:05:48.340000 0:05:53.230000 vocabulary representing\n",
      "0:05:50.830000 0:05:54.430000 words vector form\n",
      "0:05:53.230000 0:05:56.470000 seven thousand\n",
      "0:05:54.430000 0:05:59.140000 seventy nine axis one would\n",
      "0:05:56.470000 0:06:01.950000 independent word\n",
      "0:05:59.140000 0:06:05.770000 independent one way\n",
      "0:06:01.950000 0:06:08.590000 starting process convert\n",
      "0:06:05.770000 0:06:10.360000 words vector space model\n",
      "0:06:08.590000 0:06:13.390000 later need start aligning\n",
      "0:06:10.360000 0:06:16.360000 vectors way vectors\n",
      "0:06:13.390000 0:06:18.340000 close enough given word make\n",
      "0:06:16.360000 0:06:22.990000 one axis see\n",
      "0:06:18.340000 0:06:24.820000 could done little later\n",
      "0:06:22.990000 0:06:26.500000 three million words\n",
      "0:06:24.820000 0:06:28.660000 carpus going three\n",
      "0:06:26.500000 0:06:31.410000 million axis pretty huge right\n",
      "0:06:28.660000 0:06:34.810000 difficult imagine beyond\n",
      "0:06:31.410000 0:06:38.020000 three axis us need able\n",
      "0:06:34.810000 0:06:41.890000 certain operation vector\n",
      "0:06:38.020000 0:06:44.530000 space model reduce number axes\n",
      "0:06:41.890000 0:06:48.130000 time bring words\n",
      "0:06:44.530000 0:06:50.680000 close enough one axis\n",
      "0:06:48.130000 0:06:53.860000 main idea vector\n",
      "0:06:50.680000 0:06:56.380000 space model mentioned right\n",
      "0:06:53.860000 0:06:59.110000 need able plot axis\n",
      "0:06:56.380000 0:07:02.680000 try minimize number axis\n",
      "0:06:59.110000 0:07:06.070000 less number variables\n",
      "0:07:02.680000 0:07:09.040000 deal talk\n",
      "0:07:06.070000 0:07:13.630000 one lectures following\n",
      "0:07:09.040000 0:07:18.790000 weeks able represent\n",
      "0:07:13.630000 0:07:24.250000 words certain way create\n",
      "0:07:18.790000 0:07:28.090000 semantically connected vectors okay\n",
      "0:07:24.250000 0:07:32.160000 want able find\n",
      "0:07:28.090000 0:07:35.290000 relationship terms documents\n",
      "0:07:32.160000 0:07:39.480000 right like mentioned earlier buying\n",
      "0:07:35.290000 0:07:43.330000 bought brought part one\n",
      "0:07:39.480000 0:07:45.370000 axis three different axes\n",
      "0:07:43.330000 0:07:47.139000 way connecting\n",
      "0:07:45.370000 0:07:51.280000 words together could\n",
      "0:07:47.139000 0:07:53.220000 sort synonyms related buy also\n",
      "0:07:51.280000 0:07:56.680000 brought part\n",
      "0:07:53.220000 0:08:04.060000 particular axis\n",
      "0:07:56.680000 0:08:06.009000 synonyms word belong\n",
      "0:08:04.060000 0:08:08.319000 mentioned earlier need able\n",
      "0:08:06.009000 0:08:14.259000 put similar items closer\n",
      "0:08:08.319000 0:08:17.409000 space structure model\n",
      "0:08:14.259000 0:08:19.180000 discovers uncovers semantic\n",
      "0:08:17.409000 0:08:23.259000 similarity words documents\n",
      "0:08:19.180000 0:08:26.110000 latent semantic domain\n",
      "0:08:23.259000 0:08:29.710000 possible translate\n",
      "0:08:26.110000 0:08:31.870000 terms documents one des\n",
      "0:08:29.710000 0:08:34.329000 moines different domain\n",
      "0:08:31.870000 0:08:40.140000 able uncover relationship\n",
      "0:08:34.329000 0:08:40.140000 look one\n",
      "0:08:40.589000 0:08:44.669000 important aspects move along\n",
      "0:08:45.240000 0:08:52.540000 develop distributed word victors\n",
      "0:08:49.120000 0:08:54.670000 dense vectors captures linear\n",
      "0:08:52.540000 0:08:56.350000 combination word vectors\n",
      "0:08:54.670000 0:08:58.209000 transform domain example\n",
      "0:08:56.350000 0:09:03.430000 transferring one domain\n",
      "0:08:58.209000 0:09:07.360000 happens transform\n",
      "0:09:03.430000 0:09:11.140000 domain words transformed\n",
      "0:09:07.360000 0:09:13.870000 different set vectors\n",
      "0:09:11.140000 0:09:16.270000 really convey real meaning\n",
      "0:09:13.870000 0:09:18.459000 word see world right\n",
      "0:09:16.270000 0:09:21.100000 front us example\n",
      "0:09:18.459000 0:09:23.410000 document seeing\n",
      "0:09:21.100000 0:09:25.630000 contains words paragraphs\n",
      "0:09:23.410000 0:09:31.079000 transform domain look\n",
      "0:09:25.630000 0:09:35.260000 like right looking\n",
      "0:09:31.079000 0:09:40.680000 converting words word vectors\n",
      "0:09:35.260000 0:09:45.370000 dense vectors\n",
      "0:09:40.680000 0:09:48.040000 important step\n",
      "0:09:45.370000 0:09:51.490000 kinds operations able get\n",
      "0:09:48.040000 0:09:53.560000 transform domain see\n",
      "0:09:51.490000 0:09:54.810000 certain latent relationship\n",
      "0:09:53.560000 0:09:59.120000 terms\n",
      "0:09:54.810000 0:10:03.240000 documents okay thereby\n",
      "0:09:59.120000 0:10:06.420000 learning word okay\n",
      "0:10:03.240000 0:10:09.090000 word represent\n",
      "0:10:06.420000 0:10:12.930000 word alone looking\n",
      "0:10:09.090000 0:10:15.750000 contextual word surrounding\n",
      "0:10:12.930000 0:10:23.279000 particular word synonyms\n",
      "0:10:15.750000 0:10:27.150000 word converted\n",
      "0:10:23.279000 0:10:31.950000 certain form using\n",
      "0:10:27.150000 0:10:34.320000 certain nlp applications okay\n",
      "0:10:31.950000 0:10:37.830000 call us word vectors\n",
      "0:10:34.320000 0:10:40.200000 dense like go\n",
      "0:10:37.830000 0:10:43.860000 detail right might look\n",
      "0:10:40.200000 0:10:45.630000 well overwhelming right go\n",
      "0:10:43.860000 0:10:47.850000 details understand\n",
      "0:10:45.630000 0:10:50.490000 importance\n",
      "0:10:47.850000 0:10:51.960000 think understand reason\n",
      "0:10:50.490000 0:10:56.790000 converting\n",
      "0:10:51.960000 0:10:59.400000 different domain going\n",
      "0:10:56.790000 0:11:02.460000 skip part right maybe\n",
      "0:10:59.400000 0:11:04.530000 last section slide talk\n",
      "0:11:02.460000 0:11:09.030000 example\n",
      "0:11:04.530000 0:11:11.580000 document assuming four\n",
      "0:11:09.030000 0:11:13.770000 words okay first document contains\n",
      "0:11:11.580000 0:11:17.940000 automobile automobile association\n",
      "0:11:13.770000 0:11:19.470000 second one car driver okay\n",
      "0:11:17.940000 0:11:23.730000 two document\n",
      "0:11:19.470000 0:11:27.150000 looking possible\n",
      "0:11:23.730000 0:11:31.380000 connect two documents certain\n",
      "0:11:27.150000 0:11:37.860000 fashion connect automobile\n",
      "0:11:31.380000 0:11:41.010000 car automatically connect\n",
      "0:11:37.860000 0:11:44.910000 driver automobile association\n",
      "0:11:41.010000 0:11:47.880000 belongs one\n",
      "0:11:44.910000 0:11:54.390000 certain kind operation want\n",
      "0:11:47.880000 0:11:57.320000 perform corpus starting\n",
      "0:11:54.390000 0:12:02.750000 beginning right\n",
      "0:11:57.320000 0:12:05.670000 started looking counting part\n",
      "0:12:02.750000 0:12:08 started looking based\n",
      "0:12:05.670000 0:12:10.160000 counting prediction\n",
      "0:12:08 0:12:14.630000 got little deeper\n",
      "0:12:10.160000 0:12:17.860000 understanding world right terms\n",
      "0:12:14.630000 0:12:20.420000 context present\n",
      "0:12:17.860000 0:12:23.270000 creating sector dense vector\n",
      "0:12:20.420000 0:12:26.540000 represent word\n",
      "0:12:23.270000 0:12:28.760000 may represent contextual word\n",
      "0:12:26.540000 0:12:31.220000 surrounding may also\n",
      "0:12:28.760000 0:12:35.840000 represent synonyms sociated\n",
      "0:12:31.220000 0:12:39.410000 okay\n",
      "0:12:35.840000 0:12:43.810000 information world\n",
      "0:12:39.410000 0:12:47.270000 make system learn associations\n",
      "0:12:43.810000 0:12:51.860000 okay enough information\n",
      "0:12:47.270000 0:12:54.920000 really convert data certain\n",
      "0:12:51.860000 0:12:59.360000 form machine use learn\n",
      "0:12:54.920000 0:13:01.070000 understand lot\n",
      "0:12:59.360000 0:13:06.410000 information even though talk\n",
      "0:13:01.070000 0:13:08.860000 huge corpus containing billion\n",
      "0:13:06.410000 0:13:11.570000 words one trillion words\n",
      "0:13:08.860000 0:13:14.360000 reason every time create\n",
      "0:13:11.570000 0:13:18.230000 sentence always form new\n",
      "0:13:14.360000 0:13:21.080000 innovative one right need\n",
      "0:13:18.230000 0:13:23.750000 part corpus contains\n",
      "0:13:21.080000 0:13:26.840000 one trillion words 1 billion words\n",
      "0:13:26.840000 0:13:37.640000 innovative fashion always\n",
      "0:13:32.410000 0:13:40.010000 lacked sufficient knowledge ok order\n",
      "0:13:37.640000 0:13:42.350000 feed system\n",
      "0:13:40.010000 0:13:45.500000 difficult get everything\n",
      "0:13:42.350000 0:13:48.320000 possible knowledge feed\n",
      "0:13:45.500000 0:13:51.980000 part knowledge base\n",
      "0:13:48.320000 0:13:55.820000 system right solve\n",
      "0:13:51.980000 0:13:59.030000 problems really lack sufficient\n",
      "0:13:55.820000 0:14:04.250000 knowledge right start reading\n",
      "0:13:59.030000 0:14:06.770000 start listening new lectures\n",
      "0:14:04.250000 0:14:11.330000 right start looking\n",
      "0:14:06.770000 0:14:14.990000 examples start looking\n",
      "0:14:11.330000 0:14:17.450000 experience gained\n",
      "0:14:14.990000 0:14:20.540000 certain things helpful\n",
      "0:14:17.450000 0:14:23.420000 terms learning\n",
      "0:14:20.540000 0:14:26.030000 examples really provide certain\n",
      "0:14:23.420000 0:14:28.040000 underlying patterns based\n",
      "0:14:26.030000 0:14:30.800000 already available corpus right\n",
      "0:14:28.040000 0:14:34.250000 example already\n",
      "0:14:30.800000 0:14:39.530000 look underlying\n",
      "0:14:34.250000 0:14:45.050000 patterns try fill\n",
      "0:14:39.530000 0:14:50.060000 missing information using\n",
      "0:14:45.050000 0:14:53.390000 buttons patterns give us\n",
      "0:14:50.060000 0:14:57.080000 gives us ability predict\n",
      "0:14:53.390000 0:15:01.430000 outcome pattern always would give\n",
      "0:14:57.080000 0:15:05.060000 disability right always\n",
      "0:15:01.430000 0:15:07.400000 build certain models claim\n",
      "0:15:05.060000 0:15:09.020000 model would able solve\n",
      "0:15:07.400000 0:15:11 certain problems model\n",
      "0:15:09.020000 0:15:14.600000 able solve every problem\n",
      "0:15:11 0:15:16.550000 keep creating new models based\n",
      "0:15:14.600000 0:15:18.470000 learned okay\n",
      "0:15:16.550000 0:15:20.450000 learning comes picture\n",
      "0:15:18.470000 0:15:22.910000 especially mission keep\n",
      "0:15:20.450000 0:15:25.580000 providing information\n",
      "0:15:22.910000 0:15:27.560000 system starts learning\n",
      "0:15:25.580000 0:15:30.230000 patterns probably creates\n",
      "0:15:27.560000 0:15:34.310000 newer model every time one example\n",
      "0:15:30.230000 0:15:36.440000 provide sure\n",
      "0:15:34.310000 0:15:39.760000 would used translation\n",
      "0:15:36.440000 0:15:42.350000 big companies around right\n",
      "0:15:39.760000 0:15:45.890000 used translation three\n",
      "0:15:42.350000 0:15:50.450000 years back used\n",
      "0:15:45.890000 0:15:55.100000 translation big companies\n",
      "0:15:50.450000 0:15:58.330000 today would see huge difference\n",
      "0:15:55.100000 0:16:02.090000 terms quantity translation\n",
      "0:15:58.330000 0:16:04.370000 systems learning move\n",
      "0:16:02.090000 0:16:08.270000 along learning one\n",
      "0:16:04.370000 0:16:12.620000 important thing order\n",
      "0:16:08.270000 0:16:16.640000 really work ambiguous world\n",
      "0:16:12.620000 0:16:20.870000 keep learning examples every\n",
      "0:16:16.640000 0:16:24.050000 day experience every day\n",
      "0:16:20.870000 0:16:26 new learning comes picture\n",
      "0:16:24.050000 0:16:29.960000 keep changing models know\n",
      "0:16:26 0:16:33.890000 model cannot stay static forever\n",
      "0:16:29.960000 0:16:36.460000 model keep changing regular basis\n",
      "0:16:33.890000 0:16:38.270000 especially machine learn models\n",
      "0:16:36.460000 0:16:43.520000 something keep changing\n",
      "0:16:38.270000 0:16:49.670000 regular basis unless data\n",
      "0:16:43.520000 0:16:52.190000 change look various linear\n",
      "0:16:49.670000 0:16:54.980000 nonlinear classifications\n",
      "0:16:52.190000 0:16:57.590000 example spoke clustering\n",
      "0:16:54.980000 0:17:03.190000 earlier right based learning\n",
      "0:16:57.590000 0:17:07.070000 start refining classifications\n",
      "0:17:03.190000 0:17:10.010000 know better way\n",
      "0:17:07.070000 0:17:13.240000 aspects keep learning\n",
      "0:17:10.010000 0:17:17.720000 move along talk\n",
      "0:17:13.240000 0:17:20.709000 certain lower level neural net models\n",
      "0:17:17.720000 0:17:24.770000 like perceptrons learning\n",
      "0:17:20.709000 0:17:27.589000 move higher levels\n",
      "0:17:24.770000 0:17:30.440000 feed forward neural network back\n",
      "0:17:27.589000 0:17:32.720000 propagation algorithm\n",
      "0:17:30.440000 0:17:34.520000 spoke vector\n",
      "0:17:32.720000 0:17:36.050000 models earlier right neural\n",
      "0:17:34.520000 0:17:38.560000 networks going looking\n",
      "0:17:36.050000 0:17:41.150000 machine learning would require\n",
      "0:17:38.560000 0:17:46.400000 data numerical form\n",
      "0:17:41.150000 0:17:49.520000 way mission neural net models\n",
      "0:17:46.400000 0:17:51.680000 usually learn patterns well\n",
      "0:17:49.520000 0:17:54.830000 patterns\n",
      "0:17:51.680000 0:17:56.390000 difficult us learn system\n",
      "0:17:54.830000 0:17:59.930000 would automatically start predicting\n",
      "0:17:56.390000 0:18:04.400000 based certain training algorithm\n",
      "0:17:59.930000 0:18:06.640000 provide important\n",
      "0:18:04.400000 0:18:09.620000 us step terms\n",
      "0:18:06.640000 0:18:12.800000 creating vector space model\n",
      "0:18:09.620000 0:18:16.190000 words start feeding\n",
      "0:18:12.800000 0:18:18.470000 vectors input neural network\n",
      "0:18:16.190000 0:18:20.750000 training process\n",
      "0:18:18.470000 0:18:24.650000 start learning outcome\n",
      "0:18:20.750000 0:18:28.430000 would really really good know\n",
      "0:18:24.650000 0:18:30.670000 learnt data pattern\n",
      "0:18:28.430000 0:18:30.670000 well\n",
      "0:18:31.929000 0:18:40.370000 spoke vector space\n",
      "0:18:37.570000 0:18:45.890000 models neural\n",
      "0:18:40.370000 0:18:48.169000 network based application word embedding\n",
      "0:18:45.890000 0:18:50.750000 comes picture whatever\n",
      "0:18:48.169000 0:18:54.130000 spoke dense vector\n",
      "0:18:50.750000 0:18:57.559000 thing machine learning\n",
      "0:18:54.130000 0:18:59.450000 okay start creating dense\n",
      "0:18:57.559000 0:19:02.390000 vectors using mirror necklace well\n",
      "0:18:59.450000 0:19:03.380000 using certain pattern provide\n",
      "0:19:02.390000 0:19:08.380000 input\n",
      "0:19:03.380000 0:19:10.730000 using would able find\n",
      "0:19:08.380000 0:19:15.590000 semantic similarity syntactic\n",
      "0:19:10.730000 0:19:18.650000 similarities\n",
      "0:19:15.590000 0:19:23.169000 something\n",
      "0:19:18.650000 0:19:23.169000 remember spoke\n",
      "0:19:32.310000 0:19:39.490000 natural language processing okay\n",
      "0:19:36.640000 0:19:42.250000 gave certain examples\n",
      "0:19:39.490000 0:19:45.220000 right things\n",
      "0:19:42.250000 0:19:49.450000 required us\n",
      "0:19:45.220000 0:19:52.090000 order get answer\n",
      "0:19:52.090000 0:19:58.480000 word parts terms need\n",
      "0:19:54.970000 0:20:00.760000 especially dense vectors\n",
      "0:19:58.480000 0:20:06.460000 probability spoke\n",
      "0:20:00.760000 0:20:08.680000 word part okay\n",
      "0:20:06.460000 0:20:10.930000 really\n",
      "0:20:08.680000 0:20:13.300000 implement things\n",
      "0:20:10.930000 0:20:16.630000 order implement\n",
      "0:20:13.300000 0:20:20.170000 would answer question\n",
      "0:20:16.630000 0:20:23.800000 okay look\n",
      "0:20:20.170000 0:20:25.390000 important thing\n",
      "0:20:23.800000 0:20:29.400000 know every time certain\n",
      "0:20:25.390000 0:20:32.950000 things try answer three things\n",
      "0:20:29.400000 0:20:36.520000 entire course trying\n",
      "0:20:32.950000 0:20:39.310000 answer three questions okay\n",
      "0:20:36.520000 0:20:42.160000 spoke right terms\n",
      "0:20:39.310000 0:20:47.610000 word vectors use\n",
      "0:20:42.160000 0:20:51.250000 mechanism linear algebraic space\n",
      "0:20:47.610000 0:20:53.560000 using method called svd singular value\n",
      "0:20:51.250000 0:21:01.180000 decomposition okay also gives\n",
      "0:20:53.560000 0:21:04.840000 dense vector using neural net\n",
      "0:21:01.180000 0:21:07.930000 also thing neural\n",
      "0:21:04.840000 0:21:15.100000 net provides better word embedding\n",
      "0:21:07.930000 0:21:16.210000 really many cases okay\n",
      "0:21:16.210000 0:21:24.090000 various models using\n",
      "0:21:19.030000 0:21:24.090000 convert words dense vectors\n",
      "0:21:24.270000 0:21:31.960000 using dense vectors possible\n",
      "0:21:28.360000 0:21:34.810000 us mentioned understand\n",
      "0:21:31.960000 0:21:38.400000 semantic similarities syntactic\n",
      "0:21:34.810000 0:21:38.400000 similarities example\n",
      "0:21:38.950000 0:21:42.720000 also remember core\n",
      "0:22:04.169000 0:22:09.640000 quote firth said\n",
      "0:22:07.960000 0:22:14.650000 word known company\n",
      "0:22:09.640000 0:22:16.630000 keeps means world always\n",
      "0:22:14.650000 0:22:22.809000 certain contextual words surrounding\n",
      "0:22:16.630000 0:22:24.580000 context word\n",
      "0:22:22.809000 0:22:29.110000 central word little\n",
      "0:22:24.580000 0:22:31.890000 different say two\n",
      "0:22:29.110000 0:22:35.590000 words somewhat similar\n",
      "0:22:31.890000 0:22:41.159000 semantically exactly\n",
      "0:22:35.590000 0:22:41.159000 means neural net model exactly\n",
      "0:22:41.520000 0:22:47.049000 captures patterns certain\n",
      "0:22:44.500000 0:22:49.240000 fashion word embedding\n",
      "0:22:47.049000 0:22:51.549000 created dense vectors created\n",
      "0:22:49.240000 0:22:54.299000 use later\n",
      "0:22:51.549000 0:22:54.299000 applications\n"
     ]
    }
   ],
   "source": [
    "file_name = 'D:\\\\captures\\\\NPTEL\\\\ANLP\\\\Week1\\mod01lec04-Vector Space models.srt'\n",
    "with open(file_name) as f_srt:\n",
    "    my_srt_parser = parse_srt_file(f_srt)\n",
    "    for srt_block in my_srt_parser:\n",
    "        print(srt_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Each subtitle entry will be treated as a document\n",
    "  - Sentences will be splitted and joined as a single sentence for easy document handling\n",
    "  \n",
    "- TO DO\n",
    "  - Some subtitle entries have single words. How to handle that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert .SRT to .VTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_srt_to_vtt('mod01lec01.srt')\n",
    "convert_srt_to_vtt('test.srt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play video from given position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video id=\"vid1\" width=\"840\" height=\"460\" controls=\"\">\n",
       "    <source src=\"mod01lec01.mp4\" type=\"video/mp4\">\n",
       "    <track label=\"English\" kind=\"subtitles\" src=\"mod01lec01.vtt\" srclang=\"en\" default=\"\">\n",
       "</video>\n",
       "<script>\n",
       "    document.getElementById('vid1').currentTime = \"100\"\n",
       "</script>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video id=\"vid1\" width=\"840\" height=\"460\" controls=\"\">\n",
    "    <source src=\"{0}\">\n",
    "    <track label=\"English\" kind=\"subtitles\" src=\"{1}\" srclang=\"en\" default=\"\">\n",
    "</video>\n",
    "<script>\n",
    "    document.getElementById('vid1').currentTime = \"{2}\"\n",
    "</script>\n",
    "\"\"\".format('mod01lec01.mp4\" type=\"video/mp4', 'mod01lec01.vtt', 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
